{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Akash Singh\n",
    "# student Id: 1060237\n",
    "# NLP Assignement 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import sys\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function remove newline char\n",
    "\n",
    "def preprocess_doc(doc):           \n",
    "    doc = doc.replace('\\n',' ')    \n",
    "    return(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function remove punctuation, stop word, url, extra space, and lemmetaize and lower casing \n",
    "\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "def lemmatizer_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return [token.lemma_.lower() for token in tokens if not token.is_stop | token.is_punct | token.is_space | token.like_url]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extract text, label, and ids from the trainig and development data set\n",
    "\n",
    "train_file = 'new_train.json'    #new expanded training data\n",
    "dev_file = 'dev.json'            #development data\n",
    "\n",
    "def process_text(file_name):\n",
    "    f = open(file_name, 'r', encoding = 'utf-8') \n",
    "    data = json.load(f)\n",
    "    text_list = []\n",
    "    id_list = []\n",
    "    label_list = []\n",
    "    for doc_id, doc_dic in data.items():\n",
    "        text = doc_dic[\"text\"] \n",
    "        text_list.append(text)\n",
    "        label = doc_dic[\"label\"]\n",
    "        label_list.append(label)\n",
    "        id_list.append(doc_id)\n",
    "    return text_list,label_list,id_list\n",
    "\n",
    "\n",
    "train_text_list, train_label_list, train_id_list = process_text(train_file)\n",
    "dev_text_list, dev_label_list, dev_id_list = process_text(dev_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extract text, and ids from the test data set\n",
    "\n",
    "test_file = 'test-unlabelled.json' #test data\n",
    "\n",
    "def process_test_file(file_name):\n",
    "    f = open(file_name, 'r', encoding = 'utf-8')\n",
    "    data = json.load(f)\n",
    "    text_list = []\n",
    "    id_list = []\n",
    "    for doc_id, doc_dic in data.items():\n",
    "        text = doc_dic[\"text\"]\n",
    "        text_list.append(text)\n",
    "        id_list.append(doc_id)\n",
    "    return text_list,id_list\n",
    "        \n",
    "\n",
    "test_text_list, test_id_list = process_test_file(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary of vectorizers for chosing optimal parameters\n",
    "\n",
    "dict_vec = {'TFIDF_1': TfidfVectorizer(ngram_range=(1,3),\n",
    "                                      max_df = 0.8, min_df = 3, \n",
    "                                      preprocessor=preprocess_doc, \n",
    "                                      tokenizer=lemmatizer_tokenizer),                            \n",
    "            'TFIDF_2': TfidfVectorizer(ngram_range=(1,3),\n",
    "                                      max_df = 0.7, min_df = 3, \n",
    "                                      preprocessor=preprocess_doc, \n",
    "                                      tokenizer=lemmatizer_tokenizer),\n",
    "            'TFIDF_3': TfidfVectorizer(ngram_range=(1,5),\n",
    "                                      max_df = 0.7, min_df = 2, \n",
    "                                      preprocessor=preprocess_doc, \n",
    "                                      tokenizer=lemmatizer_tokenizer),\n",
    "            'TFIDF_4': TfidfVectorizer(ngram_range=(1,7),\n",
    "                                      max_df = 0.7, min_df = 1, \n",
    "                                      preprocessor=preprocess_doc, \n",
    "                                      tokenizer=lemmatizer_tokenizer),\n",
    "            '1_BOWV': CountVectorizer(ngram_range=(1,3),\n",
    "                                      max_df = 0.8, min_df = 3,\n",
    "                                      preprocessor=preprocess_doc, \n",
    "                                      tokenizer=lemmatizer_tokenizer),\n",
    "            '2_BOWV': CountVectorizer(ngram_range=(1,3),\n",
    "                                      max_df = 0.7, min_df = 3,\n",
    "                                      preprocessor=preprocess_doc, \n",
    "                                      tokenizer=lemmatizer_tokenizer),\n",
    "            '3_BOWV': CountVectorizer(ngram_range=(1,5),\n",
    "                                      max_df = 0.7, min_df = 2,\n",
    "                                      preprocessor=preprocess_doc, \n",
    "                                      tokenizer=lemmatizer_tokenizer),\n",
    "            '4_BOWV': CountVectorizer(ngram_range=(1,7),\n",
    "                                      max_df = 0.7, min_df = 1,\n",
    "                                      preprocessor=preprocess_doc, \n",
    "                                      tokenizer=lemmatizer_tokenizer),\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary of different machine learning models for chosing best performer\n",
    "\n",
    "dict_model = {'Naive Bayes': MultinomialNB(),\n",
    "              'Logistic Regression': LogisticRegression(),\n",
    "              'SGD': SGDClassifier(),\n",
    "              'LinearSVC': LinearSVC(),\n",
    "              'Decision Tree': DecisionTreeClassifier(max_depth=6),    \n",
    "              'XGBoost': XGBClassifier(max_depth=6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\akash singh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "c:\\users\\akash singh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Model_Name Vec_Name        F1  Precision  Recall\n",
      "0           Naive Bayes  TFIDF_1  0.704225   0.543478    1.00\n",
      "1   Logistic Regression  TFIDF_1  0.806723   0.695652    0.96\n",
      "2                   SGD  TFIDF_1  0.810345   0.712121    0.94\n",
      "3             LinearSVC  TFIDF_1  0.820513   0.716418    0.96\n",
      "4         Decision Tree  TFIDF_1  0.786885   0.666667    0.96\n",
      "5               XGBoost  TFIDF_1  0.753846   0.612500    0.98\n",
      "6           Naive Bayes  TFIDF_2  0.704225   0.543478    1.00\n",
      "7   Logistic Regression  TFIDF_2  0.806723   0.695652    0.96\n",
      "8                   SGD  TFIDF_2  0.817391   0.723077    0.94\n",
      "9             LinearSVC  TFIDF_2  0.820513   0.716418    0.96\n",
      "10        Decision Tree  TFIDF_2  0.780488   0.657534    0.96\n",
      "11              XGBoost  TFIDF_2  0.753846   0.612500    0.98\n",
      "12          Naive Bayes  TFIDF_3  0.671141   0.505051    1.00\n",
      "13  Logistic Regression  TFIDF_3  0.806723   0.695652    0.96\n",
      "14                  SGD  TFIDF_3  0.820513   0.716418    0.96\n",
      "15            LinearSVC  TFIDF_3  0.820513   0.716418    0.96\n",
      "16        Decision Tree  TFIDF_3  0.796748   0.671233    0.98\n",
      "17              XGBoost  TFIDF_3  0.771654   0.636364    0.98\n",
      "18          Naive Bayes  TFIDF_4  0.666667   0.500000    1.00\n",
      "19  Logistic Regression  TFIDF_4  0.709220   0.549451    1.00\n",
      "20                  SGD  TFIDF_4  0.771654   0.636364    0.98\n",
      "21            LinearSVC  TFIDF_4  0.765625   0.628205    0.98\n",
      "22        Decision Tree  TFIDF_4  0.775862   0.681818    0.90\n",
      "23              XGBoost  TFIDF_4  0.764228   0.643836    0.94\n",
      "24          Naive Bayes   1_BOWV  0.776860   0.661972    0.94\n",
      "25  Logistic Regression   1_BOWV  0.818182   0.750000    0.90\n",
      "26                  SGD   1_BOWV  0.776860   0.661972    0.94\n",
      "27            LinearSVC   1_BOWV  0.849057   0.803571    0.90\n",
      "28        Decision Tree   1_BOWV  0.777778   0.644737    0.98\n",
      "29              XGBoost   1_BOWV  0.763359   0.617284    1.00\n",
      "30          Naive Bayes   2_BOWV  0.776860   0.661972    0.94\n",
      "31  Logistic Regression   2_BOWV  0.818182   0.750000    0.90\n",
      "32                  SGD   2_BOWV  0.764228   0.643836    0.94\n",
      "33            LinearSVC   2_BOWV  0.849057   0.803571    0.90\n",
      "34        Decision Tree   2_BOWV  0.774194   0.648649    0.96\n",
      "35              XGBoost   2_BOWV  0.763359   0.617284    1.00\n",
      "36          Naive Bayes   3_BOWV  0.714286   0.555556    1.00\n",
      "37  Logistic Regression   3_BOWV  0.818182   0.750000    0.90\n",
      "38                  SGD   3_BOWV  0.764228   0.643836    0.94\n",
      "39            LinearSVC   3_BOWV  0.849057   0.803571    0.90\n",
      "40        Decision Tree   3_BOWV  0.777778   0.644737    0.98\n",
      "41              XGBoost   3_BOWV  0.763359   0.617284    1.00\n",
      "42          Naive Bayes   4_BOWV  0.666667   0.500000    1.00\n",
      "43  Logistic Regression   4_BOWV  0.818182   0.750000    0.90\n",
      "44                  SGD   4_BOWV  0.796610   0.691176    0.94\n",
      "45            LinearSVC   4_BOWV  0.811321   0.767857    0.86\n",
      "46        Decision Tree   4_BOWV  0.774194   0.648649    0.96\n",
      "47              XGBoost   4_BOWV  0.763359   0.617284    1.00\n"
     ]
    }
   ],
   "source": [
    "#Training the list of models to pick best one's out of it.\n",
    "\n",
    "dict_res = defaultdict(list)\n",
    "for vec_name, vectorizer in dict_vec.items():                   #looping through list of vectorizers\n",
    "    X_train_vec = vectorizer.fit_transform(train_text_list)\n",
    "    X_dev_vec  = vectorizer.transform(dev_text_list)\n",
    "    for mod_name, model in dict_model.items():                  #Looping through list of models\n",
    "        model.fit(X_train_vec, train_label_list);\n",
    "        y_pred = model.predict(X_dev_vec)\n",
    "        precision = precision_score(dev_label_list, y_pred)\n",
    "        recall = recall_score(dev_label_list, y_pred)\n",
    "        f1 = f1_score(dev_label_list, y_pred)\n",
    "        dict_res['Model_Name'].append(mod_name)\n",
    "        dict_res['Vec_Name'].append(vec_name)\n",
    "        dict_res['F1'].append(f1)\n",
    "        dict_res['Precision'].append(precision)\n",
    "        dict_res['Recall'].append(recall)\n",
    "    df_res = pd.DataFrame(dict_res)\n",
    "print(df_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar plot for F1-score on development set, for different classifiers with CountVectorizier= 2_BOWV \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "F1_2_BOWV = [0.7768,0.8181,0.7643,0.8490,0.7740,0.7633]    #F1 score for CountVectorizer 2_BOWV\n",
    "plt.bar([num + 0.25 for num in range(len(F1_2_BOWV))], F1_2_BOWV,0.5)\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('F1-Score with Countvect 2_BOWV, by classifier')\n",
    "plt.ylim([0.0,1])\n",
    "plt.xticks([num + 0.5 for num in range(len(F1_2_BOWV))], ('NB', 'LR', 'SGD', 'LSVC', 'DT', 'XGB'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******** Logistic Regression For i = 0.8 threshold******\n",
      " \n",
      "            Model_Name Vec_Name        F1  Precision  Recall\n",
      "0  Logistic Regression   2_BOWV  0.841121   0.789474     0.9\n",
      "1            LinearSVC   2_BOWV  0.841121   0.789474     0.9\n"
     ]
    }
   ],
   "source": [
    "#Using Grid-Search, Threshold value tuining and custom stop word list for performance improvemnt of LR & LSVC classifiers\n",
    "\n",
    "dict_res = defaultdict(list)\n",
    "\n",
    "#List of words from development set instaces which are wrongly classified as positive\n",
    "\n",
    "stop_words_new = ['predatory', 'accuse', 'emission', 'reduction', 'engage', 'pragmatist', \n",
    "                  'scepticism', 'dramatic', 'wedge', 'catastrophe', 'carbon', 'tackle', \n",
    "                  'polluter', 'decarbonising', 'plastic', 'package', 'heat', 'rise', \n",
    "                  'decarbonise', 'coal', 'market', 'globe', 'fire', 'power', 'incentivise', \n",
    "                  'underwrite', 'exporter', 'metallurgical', 'export', 'crisis', 'global', 'warm', \n",
    "                  'paris', 'agreement', 'hypnotist', 'dispute', 'calamity', 'hypnotism', 'weaponises', \n",
    "                  'party', 'opponent', 'blithely', 'unavoidable', 'inexorable', 'dangerous', 'gillard', \n",
    "                  'incomprehensible', 'gamble', 'misdirection', 'strike', 'feminism', 'movement', \n",
    "                  'massively', 'striker', 'conversation', 'sensational', 'zealotry', 'fabricate', \n",
    "                  'thunberg', 'tragedy', 'furiously', 'dominate', 'destructive', 'argue', 'bushfire',\n",
    "                  'widespread', 'environmental', 'destruction', 'defunding', 'turnbull', 'innovation', \n",
    "                  'agendum', 'economic', 'lapdog', 'puppet', 'fund', 'immune', 'coalition', 'starve', 'unobserved',\n",
    "                  'unreported', 'unprecedented', 'ferocious', 'untroubled', 'alarm', 'objection', 'wealthy', \n",
    "                  'collectively', 'dioxide', 'consequence', 'agendum', 'oil', 'rich', 'fragile', 'resist',\n",
    "                  'fossil', 'fuel', 'unlawful', 'crowdfunding', 'incompatible', 'lobbyist']\n",
    "\n",
    "#Final vectorizier with pipiline of process\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3),\n",
    "                             max_df = 0.7, min_df = 3,\n",
    "                             preprocessor=preprocess_doc, \n",
    "                             tokenizer=lemmatizer_tokenizer,\n",
    "                             analyzer='word',\n",
    "                             stop_words = stop_words_new)\n",
    "vec_name = '2_BOWV' \n",
    "\n",
    "#Custome parameter list for LR & LSVC classifiers used with Grid-Search\n",
    "\n",
    "params_logit = [{'penalty': ['l2']},          #LR parameters used in pipeline with Grid-search\n",
    "                {'solver': ['lbfgs']},\n",
    "                {'C':[1]},\n",
    "               {'max_iter':[100]}]\n",
    "\n",
    "params_lsvc = [{'loss':['squared_hinge']},    #LSVC parameters used in pipeline with Grid-search\n",
    "              {'penalty': ['l2']},\n",
    "              {'random_state':[42]},\n",
    "             {'max_iter':[1000]}]\n",
    "\n",
    "\n",
    "\n",
    "X_train_new = vectorizer.fit_transform(train_text_list)\n",
    "X_dev_new  = vectorizer.transform(dev_text_list)\n",
    "    \n",
    "# Logistic Regression\n",
    "gs_logit = GridSearchCV(LogisticRegression(), params_logit,\n",
    "                        scoring='f1', error_score=0.0)\n",
    "gs_logit_fit = gs_logit.fit(X_train_new, train_label_list)\n",
    "pred_proba_df = pd.DataFrame(gs_logit_fit.predict_proba(X_dev_new))\n",
    "\n",
    "#threshold_list = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.99]\n",
    "threshold_list = [0.80]\n",
    "for i in threshold_list:\n",
    "    print ('\\n******** Logistic Regression For i = {} threshold******'.format(i))\n",
    "    y_pred_logit = pred_proba_df.applymap(lambda x: 1 if x>i else 0)\n",
    "    precision_logit = precision_score(dev_label_list, y_pred_logit.iloc[:,1])\n",
    "    recall_logit = recall_score(dev_label_list, y_pred_logit.iloc[:,1])\n",
    "    f1_logit = f1_score(dev_label_list, y_pred_logit.iloc[:,1])\n",
    "    accu_logit = accuracy_score(dev_label_list,y_pred_logit.iloc[:,1])\n",
    "    dict_res['Model_Name'].append('Logistic Regression')\n",
    "    dict_res['Vec_Name'].append(vec_name)\n",
    "    dict_res['F1'].append(f1_logit)\n",
    "    dict_res['Precision'].append(precision_logit)\n",
    "    dict_res['Recall'].append(recall_logit)\n",
    "    \n",
    "    \n",
    "#LinearSVC Classifier\n",
    "gs_lsvm = GridSearchCV(LinearSVC(), params_lsvc,\n",
    "                       scoring='f1')\n",
    "gs_lsvm.fit(X_train_new, train_label_list)\n",
    "y_pred_lsvm = gs_lsvm.predict(X_dev_new)\n",
    "precision_lsvm = precision_score(dev_label_list, y_pred_lsvm)\n",
    "recall_lsvm = recall_score(dev_label_list, y_pred_lsvm)\n",
    "f1_lsvm = f1_score(dev_label_list, y_pred_lsvm)\n",
    "accu_lsvm = accuracy_score(dev_label_list,y_pred_lsvm)\n",
    "dict_res['Model_Name'].append('LinearSVC')\n",
    "dict_res['Vec_Name'].append(vec_name)\n",
    "dict_res['F1'].append(f1_lsvm)\n",
    "dict_res['Precision'].append(precision_lsvm)\n",
    "dict_res['Recall'].append(recall_lsvm)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "print(\" \")\n",
    "df_res = pd.DataFrame(dict_res)\n",
    "print(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating predictions on test data set\n",
    "\n",
    "final_vec = CountVectorizer(ngram_range=(1,3),\n",
    "                            max_df = 0.7, min_df = 3,\n",
    "                            preprocessor=preprocess_doc, \n",
    "                            tokenizer=lemmatizer_tokenizer,\n",
    "                            analyzer='word',\n",
    "                            stop_words = stop_words_new)\n",
    "\n",
    "logit_params_final = [{'penalty': ['l2']},             #LR parameters used in pipeline with Grid-search\n",
    "                      {'solver': ['lbfgs']},\n",
    "                      {'C':[1]},\n",
    "                      {'max_iter':[100]}]\n",
    "\n",
    "lsvm_params_final = [{'loss':['squared_hinge']},       #LSVC parameters used in pipeline with Grid-search\n",
    "                     {'penalty': ['l2']},\n",
    "                     {'random_state':[42]},\n",
    "                     {'max_iter':[1000]}]\n",
    "\n",
    "X_train_fin = final_vec.fit_transform(train_text_list)\n",
    "X_test_fin  = final_vec.transform(test_text_list)\n",
    "\n",
    "#Logistic Regression\n",
    "i = 0.8                                                 #threshold value final\n",
    "gs_logit = GridSearchCV(LogisticRegression(), logit_params_final, \n",
    "                        scoring='f1', error_score=0.0)\n",
    "gs_logit_fit = gs_logit.fit(X_train_fin, train_label_list)\n",
    "pred_proba_df = pd.DataFrame(gs_logit_fit.predict_proba(X_test_fin))\n",
    "\n",
    "##generating prediction on test data for LR\n",
    "test_pred_gs_logit = pred_proba_df.applymap(lambda x: 1 if x>i else 0)     #applying threshold value \n",
    "test_pred_gs_logit = test_pred_gs_logit.iloc[:,1]\n",
    "\n",
    "\n",
    "#LinearSVC\n",
    "gs_lsvm = GridSearchCV(LinearSVC(), lsvm_params_final,\n",
    "                            scoring='f1')\n",
    "gs_lsvm_fit = gs_lsvm.fit(X_train_fin, train_label_list)\n",
    "\n",
    "##generating prediction on test data for LSVC\n",
    "test_pred_gs_lsvm = gs_lsvm_fit.predict(X_test_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating test-output.json file for codalab upload\n",
    "\n",
    "id_dict = {}\n",
    "label_dict = {}\n",
    "for ids, label in zip(test_id_list,test_pred_gs_logit):\n",
    "    id_dict[ids] = {\"label\":label}\n",
    "\n",
    "def convert(o):\n",
    "    if isinstance(o, np.int32): return int(o)  \n",
    "    raise TypeError\n",
    "    \n",
    "with open('test-output.json', 'w') as outfile:\n",
    "    json.dump(id_dict, outfile,default=convert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
